import argparse
import os
import shutil
import requests
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer
from chromadb import PersistentClient
from tqdm import tqdm
import subprocess
import warnings
from pypdf.errors import PdfReadWarning
import torch
import open_clip
from PIL import Image

import os

# å°†æ¨¡å‹ç¼“å­˜é‡å®šå‘åˆ° /data åˆ†åŒº
os.environ['HF_HOME'] = '/data/pengfei/.cache/huggingface'
os.environ['XDG_CACHE_HOME'] = '/data/pengfei/.cache'

# ======================
# é…ç½®
# ======================
PAPER_DIR = "data/papers"
RAW_DIR = "data/papers_raw"
DB_DIR = "data/index"
COLLECTION_NAME = "papers"
IMAGE_DIR = "data/images"
IMAGE_COLLECTION_NAME = "images"

OLLAMA_URL = "http://127.0.0.1:11434/api/generate"
LLM_MODEL = "qwen2:1.5b"

EMB_MODEL = SentenceTransformer("all-MiniLM-L6-v2")

client = PersistentClient(path=DB_DIR)

# åŠ è½½ CLIP æ¨¡å‹
# CLIP ä½¿å¾—æ–‡å­—å’Œå›¾ç‰‡å¯ä»¥ç›´æ¥å¯¹æ¯”
clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')
clip_tokenizer = open_clip.get_tokenizer('ViT-B-32')

def get_image_collection():
    return client.get_or_create_collection(IMAGE_COLLECTION_NAME)

# ======================
# ç´¢å¼•å›¾ç‰‡
# ======================
def index_images(folder_path):
    """æ‰«ææ–‡ä»¶å¤¹å¹¶å¯¹å›¾ç‰‡è¿›è¡Œå‘é‡åŒ–"""
    col = get_image_collection()
    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
    for filename in tqdm(image_files, desc="Indexing Images"):
        path = os.path.join(folder_path, filename)
        try:
            image = preprocess(Image.open(path)).unsqueeze(0)
            with torch.no_grad():
                # æå–å›¾ç‰‡å‘é‡
                image_features = clip_model.encode_image(image)
                image_features /= image_features.norm(dim=-1, keepdim=True)
                emb = image_features.tolist()[0]
            
            col.add(
                embeddings=[emb],
                metadatas=[{"path": path, "filename": filename}],
                ids=[filename]
            )
        except Exception as e:
            print(f"Error indexing {filename}: {e}")

# ======================
# ä»¥æ–‡æœå›¾
# ======================
def get_image_collection():
    # æ˜¾å¼æŒ‡å®šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé¿å… L2 å¯¼è‡´è·ç¦»è¶…è¿‡ 1
    return client.get_or_create_collection(
        IMAGE_COLLECTION_NAME, 
        metadata={"hnsw:space": "cosine"}
    )


def search_image_by_text(query, top_k=1):
    col = get_image_collection()
    
    # æ–‡æœ¬å‘é‡åŒ–å¹¶å½’ä¸€åŒ–
    text = clip_tokenizer([query])
    with torch.no_grad():
        text_features = clip_model.encode_text(text)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        q_emb = text_features.cpu().numpy().tolist()[0]
    
    res = col.query(
        query_embeddings=[q_emb],
        n_results=top_k,
        include=["metadatas"]
    )

    if not res["metadatas"] or not res["metadatas"][0]:
        return
    print(f"\n>>> ğŸ–¼ï¸ æœç´¢æè¿° '{query}' çš„åŒ¹é…ç»“æœ:")
    print("-" * 60)
    for meta in res["metadatas"][0]:
        print(meta['filename'])
    print("-" * 60)
# ======================
# å·¥å…·
# ======================
def get_collection():
    return client.get_or_create_collection(COLLECTION_NAME)

def load_pdf_by_page(pdf_path):
    reader = PdfReader(pdf_path)
    pages = []
    for i, page in enumerate(reader.pages):
        text = page.extract_text()
        if text:
            pages.append({"page": i + 1, "text": text})
    return pages

def chunk_text(text, size=500, overlap=100):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunks.append(" ".join(words[i:i+size]))
        i += size - overlap
    return chunks



def call_qwen(prompt, model="qwen2:1.5b"):
    try:
        result = subprocess.run(
            ["ollama", "run", model, prompt],
            capture_output=True,
            text=True,
            timeout=300
        )
        if result.returncode != 0:
            raise RuntimeError(f"Ollama CLI Error: {result.stderr}")
        return result.stdout.strip()
    except Exception as e:
        raise RuntimeError(f"Ollama call failed: {e}")


# ======================
# è‡ªåŠ¨åˆ†ç±»
# ======================
def classify_paper(text):
    prompt = f"""
è¯·åˆ¤æ–­ä»¥ä¸‹è®ºæ–‡ä¸»è¦ç ”ç©¶æ–¹å‘ï¼Œåªè¿”å›ä¸€ä¸ªæ ‡ç­¾ï¼š
CV / NLP / RL / Other

è®ºæ–‡å†…å®¹ï¼š
{text[:2000]}
"""
    label = call_qwen(prompt)
    return label if label in ["CV", "NLP", "RL"] else "Other"

# ======================
# æ·»åŠ è®ºæ–‡
# ======================
def add_paper(pdf_path):
    pages = load_pdf_by_page(pdf_path)
    full_text = " ".join(p["text"] for p in pages)

    category = classify_paper(full_text)
    save_dir = os.path.join(PAPER_DIR, category)
    os.makedirs(save_dir, exist_ok=True)

    filename = os.path.basename(pdf_path)
    save_path = os.path.join(save_dir, filename)
    shutil.copy(pdf_path, save_path)

    collection = get_collection()

    docs, embs, metas, ids = [], [], [], []

    for p in tqdm(pages, desc="Indexing"):
        for i, chunk in enumerate(chunk_text(p["text"])):
            docs.append(chunk)
            embs.append(EMB_MODEL.encode(chunk).tolist())
            metas.append({"file": filename, "page": p["page"], "category": category})
            ids.append(f"{filename}_p{p['page']}_c{i}")

    collection.add(documents=docs, embeddings=embs, metadatas=metas, ids=ids)

    print(f"[DONE] {filename} â†’ {category}, chunks={len(docs)}")

# ======================
# è¯­ä¹‰æœç´¢ + è¿”å›æœ€ç›¸å…³è®ºæ–‡åŠé¡µç 
# ======================
def search_paper(query):
    q_emb = EMB_MODEL.encode(query).tolist()
    col = get_collection()

    # æŸ¥è¯¢ top 1 æœ€ç›¸å…³è®ºæ–‡
    res = col.query(
        query_embeddings=[q_emb],
        n_results=1,
        include=["documents", "metadatas", "distances"]
    )

    if not res["documents"][0]:
        print("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚")
        return

    documents = res["documents"][0]
    metadatas = res["metadatas"][0]

    # è·å–æœ€ç›¸å…³è®ºæ–‡çš„æ–‡ä»¶å
    top_file = metadatas[0]["file"]

    # æ”¶é›†å±äºè¯¥è®ºæ–‡çš„æ‰€æœ‰é¡µç 
    pages = set()
    for meta in metadatas:
        if meta["file"] == top_file:
            pages.add(meta["page"])
    pages = sorted(pages)

    # è¾“å‡ºç»“æœ
    print(f"\nã€æœ€ç›¸å…³è®ºæ–‡ã€‘{top_file}")
    print("ã€ç›¸å…³é¡µç ã€‘")
    for p in pages:
        print("-", p)




# ======================
# è¯­ä¹‰æœç´¢ + æ–‡ä»¶ç´¢å¼• 
# ======================
def list_files(query, top_k_files=5, search_depth=50):
    """
    åˆ—å‡ºä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡ä»¶åˆ—è¡¨ã€‚
    åŸç†ï¼šæ£€ç´¢ Top N ä¸ªç›¸å…³ç‰‡æ®µï¼Œç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶åŒ…å«çš„ç‰‡æ®µæ•°é‡ï¼Œä»¥æ­¤ä½œä¸ºç›¸å…³æ€§æ‰“åˆ†ã€‚
    
    Args:
        query (str): æœç´¢è¯­å¥
        top_k_files (int): è¿”å›çš„æ–‡ä»¶æ•°é‡
        search_depth (int): æ£€ç´¢çš„ç‰‡æ®µæ€»æ± å¤§å°ï¼ˆè¶Šå¤§è¶Šç²¾å‡†ï¼Œä½†ç•¥æ…¢ï¼‰
    """
    q_emb = EMB_MODEL.encode(query).tolist()
    col = get_collection()

    # 1. æ‰©å¤§æœç´¢èŒƒå›´ï¼Œè·å–æ›´å¤šç›¸å…³ç‰‡æ®µä»¥ç»Ÿè®¡åˆ†å¸ƒ
    res = col.query(
        query_embeddings=[q_emb],
        n_results=search_depth, 
        include=["metadatas", "distances"]
    )

    if not res["metadatas"] or not res["metadatas"][0]:
        print(f"æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„è®ºæ–‡ã€‚")
        return []

    # 2. ç»Ÿè®¡æ–‡ä»¶å‡ºç°é¢‘ç‡ (Hit Count)
    file_stats = {}  # {filename: {'count': int, 'category': str}}
    
    for meta in res["metadatas"][0]:
        fname = meta["file"]
        cat = meta.get("category", "Unknown")
        
        if fname not in file_stats:
            file_stats[fname] = {'count': 0, 'category': cat}
        
        file_stats[fname]['count'] += 1

    # 3. æ’åºï¼šæŒ‰å‘½ä¸­æ¬¡æ•°å€’åº (å‘½ä¸­æ¬¡æ•°è¶Šå¤šï¼Œè¯´æ˜æ–‡ä¸­ç›¸å…³å†…å®¹è¶Šå¤š)
    ranked_files = sorted(file_stats.items(), key=lambda x: x[1]['count'], reverse=True)

    # 4. æ ¼å¼åŒ–è¾“å‡º
    print(f"\n>>> ğŸ” å…³é”®è¯ '{query}' çš„æœç´¢ç»“æœ (Top {top_k_files}):")
    print("-" * 60)
    print(f"{'Category':<10} | {'Rel. Score':<10} | {'Filename'}")
    print("-" * 60)

    result_filenames = []
    
    for fname, stats in ranked_files[:top_k_files]:
        # ç®€å•çš„å½’ä¸€åŒ–åˆ†æ•°å±•ç¤º (åŸºäº search_depth)
        score_display = f"{stats['count']}" 
        print(f"[{stats['category']:<8}] | {score_display:<10} | {fname}")
        result_filenames.append(fname)
        
    print("-" * 60)
    
    return result_filenames






# ======================
# æ‰¹é‡æ•´ç†
# ======================
def organize_all(folder):
    for f in os.listdir(folder):
        if f.endswith(".pdf"):
            add_paper(os.path.join(folder, f))

# ======================
# CLI
# ======================
def main():
    parser = argparse.ArgumentParser()
    sub = parser.add_subparsers(dest="cmd")

    add_parser = sub.add_parser("add_paper")
    add_parser.add_argument("path")
    add_parser.add_argument(
        "--topics",
        type=str,
        default="CV,NLP,RL",
    )

    sub.add_parser("search_paper").add_argument("query")
    sub.add_parser("organize_all").add_argument("folder")
    list_parser = sub.add_parser("list_files")
    list_parser.add_argument("query", type=str, help="æœç´¢å…³é”®è¯")
    list_parser.add_argument("--top_k", type=int, default=5, help="æ˜¾ç¤ºç»“æœæ•°é‡")
    


    # ç´¢å¼•å›¾ç‰‡çš„å‘½ä»¤
    img_idx_parser = sub.add_parser("index_images")
    img_idx_parser.add_argument("folder", help="å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„")
    
    # æœç´¢å›¾ç‰‡çš„å‘½ä»¤
    img_search_parser = sub.add_parser("search_image")
    img_search_parser.add_argument("query", help="æœç´¢å…³é”®è¯")
    img_search_parser.add_argument("--top_k", type=int, default=1, help="è¾“å‡ºç»“æœæ•°é‡")

    args = parser.parse_args()
    if args.cmd == "add_paper":
        add_paper(args.path)
    elif args.cmd == "search_paper":
        search_paper(args.query)
    elif args.cmd == "list_files":
        list_files(args.query, top_k_files=args.top_k)
    elif args.cmd == "organize_all":
        organize_all(args.folder)
    elif args.cmd == "index_images":
        index_images(args.folder)
    elif args.cmd == "search_image":
        search_image_by_text(args.query, top_k=args.top_k)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
